import json
import logging
import os
import sys
import base64
from dotenv import load_dotenv

# Load environment variables FIRST before any other imports
load_dotenv()

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse, RedirectResponse, FileResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel

# LangChain imports
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.messages import BaseMessage
from langchain_openai import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)

# Memory Manager import
from . import memory_manager

# â”€â”€â”€ ENV + LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logger = logging.getLogger("jewelrybox_ai")
logger.setLevel(logging.INFO)

# â”€â”€â”€ PATHS & TEMPLATES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ROOT = os.path.dirname(os.path.abspath(__file__))
TEMPLATE_DIR = os.path.join(ROOT, "templates")
templates = Jinja2Templates(directory=TEMPLATE_DIR)

# â”€â”€â”€ LOAD PROMPT CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

prompt_file = os.path.join(ROOT, "prompts", "prompt.json")
try:
    with open(prompt_file, "r", encoding="utf-8") as f:
        AGENT_ROLES = json.load(f)
except FileNotFoundError:
    logger.error(f"Prompt file not found at {prompt_file}")
    sys.exit("Prompt configuration is missing. Aborting startup.")

# â”€â”€â”€ LOAD KNOWLEDGEBASE CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

kb_file = os.path.join(ROOT, "prompts", "diamond_family_kb.json")
try:
    with open(kb_file, "r", encoding="utf-8") as f:
        DIAMOND_KB = json.load(f)["diamond_family_kb"]
except FileNotFoundError:
    logger.warning(f"Knowledgebase file not found at {kb_file}")
    DIAMOND_KB = {}


# â”€â”€â”€ ENCODE AVATAR IMAGE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

img_path = os.path.join(ROOT, "images", "male_avatar.png.png")
if os.path.exists(img_path):
    with open(img_path, "rb") as img:
        IMG_URI = "data:image/png;base64," + base64.b64encode(img.read()).decode()
else:
    logger.warning(f"Image not found at {img_path}, using fallback.")
    IMG_URI = "https://via.placeholder.com/60x60/0066cc/ffffff?text=ðŸ’Ž"

# â”€â”€â”€ FASTAPI SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(title="JewelryBox.AI Assistant")
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("ALLOWED_ORIGINS", "").split(","),
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# â”€â”€â”€ LLM + MEMORY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

memory = InMemoryChatMessageHistory(return_messages=True)
llm = ChatOpenAI(model="gpt-4o-mini", max_tokens=1024, temperature=0.9)

system_data = AGENT_ROLES["jewelry_ai"][0]["systemPrompt"]

# â”€â”€â”€ INJECT DESIGNER LISTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

designer_guardrails = DIAMOND_KB.get("productsDesigners", {}).get("guardrails", {}).get("designerVerification", {})

allowed_designers = designer_guardrails.get("allowedDesigners", [])
denied_designers = designer_guardrails.get("deniedDesigners", [])
designer_response_policy = designer_guardrails.get("responsePolicy", "If unsure, ask the user clarifying questions.")

formatted_allowed = "\nâ€¢ " + "\nâ€¢ ".join(sorted(allowed_designers))
formatted_denied = "\nâ€¢ " + "\nâ€¢ ".join(sorted(denied_designers))

logger.info(f"Loaded {len(allowed_designers)} allowed designers and {len(denied_designers)} denied designers into system prompt.")

# Format system message block
system_prompt = f"""You are {system_data['identity']}, serving as {system_data['role']}.

Tone: {system_data['tone']}

{chr(10).join(system_data['description'])}

Domains of Expertise:
{chr(10).join(system_data['knowledgeDomains'])}

Customer Service Principles:
{chr(10).join(system_data['customerServiceExcellence'])}

Anti-Looping Guidelines:
Principles:
{chr(10).join(system_data['antiLooping']['principles'])}
Variation Techniques:
{chr(10).join(system_data['antiLooping']['variationTechniques'])}
Context Awareness:
{chr(10).join(system_data['antiLooping']['contextAwareness'])}

Style Guide:
Formatting:
{chr(10).join(system_data['styleGuide']['formatting'])}
Response Structure Principles:
{chr(10).join(system_data['styleGuide']['responseStructure']['principles'])}
Formatting Guidelines:
â€¢ Headers: {system_data['styleGuide']['responseStructure']['formatting']['headers']}
â€¢ Emphasis: {system_data['styleGuide']['responseStructure']['formatting']['emphasis']}
â€¢ Lists: {system_data['styleGuide']['responseStructure']['formatting']['lists']}
â€¢ Spacing: {system_data['styleGuide']['responseStructure']['formatting']['spacing']}
â€¢ Structure: {system_data['styleGuide']['responseStructure']['formatting']['structure']}
Language:
{chr(10).join(system_data['styleGuide']['language'])}

Pricing Guidance:
{chr(10).join(system_data['pricingGuidance'])}

Care & Maintenance:
{chr(10).join(system_data['careAndMaintenance'])}

Gift Guidance:
{chr(10).join(system_data['giftGuidance'])}

Closing Style:
{chr(10).join(system_data['signatureCloser'])}

Landmine Detection and Diffusion Strategy:
{system_data['landmineDetectionAndDiffusion']['strategy']}

Risk Categories and Handling:
â€¢ Ethical Sourcing: {system_data['landmineDetectionAndDiffusion']['categories']['ethicalSourcing']}
â€¢ Pricing Risks: {system_data['landmineDetectionAndDiffusion']['categories']['pricingRisks']}
â€¢ Lab Diamond Confusion: {system_data['landmineDetectionAndDiffusion']['categories']['labDiamondConfusion']}
â€¢ Certification Claims: {system_data['landmineDetectionAndDiffusion']['categories']['certificationClaims']}
â€¢ Care and Cleaning: {system_data['landmineDetectionAndDiffusion']['categories']['careAndCleaning']}
â€¢ Service Scope: {system_data['landmineDetectionAndDiffusion']['categories']['serviceScope']}
â€¢ Memory Mismatch: {system_data['landmineDetectionAndDiffusion']['categories']['memoryMismatch']}
â€¢ Location Mismatch: {system_data['landmineDetectionAndDiffusion']['categories']['locationMismatch']}

Designer Knowledge Guardrails:

Designers Carried by Diamond Family:
{formatted_allowed}

Designers NOT Carried:
{formatted_denied}

Response Policy:
{designer_response_policy}

Knowledgebase Profile:
â€¢ Location: {DIAMOND_KB.get('businessProfile', {}).get('primaryLocation', 'N/A')}
â€¢ Website: {DIAMOND_KB.get('businessProfile', {}).get('contact', {}).get('website', 'N/A')}
â€¢ Appointment Link: {DIAMOND_KB.get('services', {}).get('scheduling', {}).get('preferredTool', 'N/A')}
â€¢ POS System: {DIAMOND_KB.get('systemTools', {}).get('POS', 'N/A')}
â€¢ CRM: {DIAMOND_KB.get('systemTools', {}).get('CRM', 'N/A')}
â€¢ Featured Event: {DIAMOND_KB.get('eventsPromotions', {}).get('calendar', [{}])[0].get('event', 'N/A')}

Tagline: {system_data['tagline']}

IMPORTANT INSTRUCTION:
{system_data['humanPrompt']}
"""

prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(system_prompt),
    MessagesPlaceholder(variable_name="history"),
    HumanMessagePromptTemplate.from_template("{user_input}")
])
chain = prompt_template | llm

# â”€â”€â”€ UNIFIED RESPONSE PROCESSOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_ai_response(user_input: str, ai_response: str) -> str:
    """
    Unified processor for both text and voice responses to ensure consistency.
    Applies URL injection and any other post-processing.
    """
    try:
        # Apply memory manager URL injection
        processed_response = memory_manager.inject_relevant_url(user_input, ai_response)
        return processed_response
    except Exception as e:
        logger.error(f"Error processing AI response: {e}")
        return ai_response  # Return original if processing fails

# â”€â”€â”€ REQUEST MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ChatRequest(BaseModel):
    user_input: str
    history: list

class VoiceTranscriptRequest(BaseModel):
    user_input: str
    ai_response: str

# â”€â”€â”€ ROOT REDIRECT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/")
async def root():
    """Redirect root URL to the voice widget"""
    return RedirectResponse(url="/voice")

# â”€â”€â”€ MAIN ROUTES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/voice", response_class=HTMLResponse)
async def voice_chat(request: Request):
    """Render the voice-enabled chat widget UI"""
    # Force HTTPS for production deployments (Render.com, etc.)
    scheme = "https" if "onrender.com" in str(request.url.netloc) or request.url.scheme == "https" else request.url.scheme
    
    return templates.TemplateResponse(
        "voice_widget.html",
        {
            "request": request,
            "chat_url": f"{scheme}://{request.url.netloc}/chat",
            "voice_session_url": f"{scheme}://{request.url.netloc}/voice/session",
            "voice_process_url": f"{scheme}://{request.url.netloc}/voice/process",
            "img_uri": IMG_URI,
        },
    )

# â”€â”€â”€ CHAT ENDPOINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def serialize_messages(messages: list[BaseMessage]):
    return [{"role": msg.type, "content": msg.content} for msg in messages]

@app.post("/chat")
async def chat(req: ChatRequest):
    try:
        history = req.history or []
        result = chain.invoke({"user_input": req.user_input, "history": history})
        reply = result.content.strip()
        
        # Use unified response processor
        reply = process_ai_response(req.user_input, reply)
        
        memory.add_user_message(req.user_input)
        memory.add_ai_message(reply)
        return JSONResponse({
            "reply": reply,
            "history": serialize_messages(memory.messages)
        })
    except Exception as e:
        logger.error(f"Error in /chat: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"error": "An internal error occurred. Please try again later."}
        )

# â”€â”€â”€ VOICE RESPONSE PROCESSOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/voice/process")
async def process_voice_response(req: VoiceTranscriptRequest):
    """
    Process voice responses to ensure they match text output formatting.
    This endpoint is called by the frontend after receiving voice responses.
    """
    try:
        # Use the same processing pipeline as text chat
        processed_response = process_ai_response(req.user_input, req.ai_response)
        
        return JSONResponse({
            "processed_response": processed_response
        })
    except Exception as e:
        logger.error(f"Error processing voice response: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"error": "Failed to process voice response"}
        )

# â”€â”€â”€ CLEAR CHAT ENDPOINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/clear_chat")
async def clear_chat():
    """
    Clear in-memory chat history for all users (global reset).
    Note: This affects all sessions since memory is global.
    """
    try:
        memory.clear()
        return JSONResponse({"status": "ok", "message": "Chat history cleared."})
    except Exception as e:
        logger.error(f"Error clearing chat history: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"error": "Failed to clear chat history."}
        )

# â”€â”€â”€ REALTIME VOICE ENDPOINTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/voice/session")
async def create_voice_session():
    """
    Create an ephemeral token for OpenAI Realtime API.
    This endpoint is called by the frontend to get a token for WebRTC connection.
    """
    try:
        import requests
        
        # Create ephemeral session token using requests
        response = requests.post(
            "https://api.openai.com/v1/realtime/sessions",
            json={
                "model": "gpt-4o-realtime-preview-2024-12-17",
                "voice": "ballad",
                "instructions": system_prompt,  # Use the same system prompt as chat
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": "whisper-1"
                },
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 500
                },
                "tools": [],  # Can add function calling later
                "tool_choice": "auto",
                "temperature": 0.8,
                "max_response_output_tokens": 4096
            },
            headers={
                "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                "Content-Type": "application/json"
            }
        )
        
        if response.status_code == 200:
            session_data = response.json()
            return JSONResponse(session_data)
        else:
            logger.error(f"Failed to create voice session: {response.text}")
            return JSONResponse(
                status_code=500,
                content={"error": "Failed to create voice session"}
            )
            
    except Exception as e:
        logger.error(f"Error creating voice session: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"error": "Failed to create voice session"}
        )

# â”€â”€â”€ WIDGET ENDPOINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/widget", response_class=HTMLResponse)
async def widget(request: Request):
    """Render the chat widget UI"""
    # Force HTTPS for production deployments (Render.com, etc.)
    scheme = "https" if "onrender.com" in str(request.url.netloc) or request.url.scheme == "https" else request.url.scheme
    
    return templates.TemplateResponse(
        "widget.html",
        {
            "request": request,
            "chat_url": f"{scheme}://{request.url.netloc}/chat",
            "img_uri": IMG_URI,
        },
    )

# â”€â”€â”€ FAVICON ENDPOINT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/favicon.ico")
async def favicon():
    """Serve the favicon"""
    favicon_path = os.path.join(ROOT, "images", "diamond.ico")
    return FileResponse(favicon_path, media_type="image/x-icon")

# â”€â”€â”€ CLI SANITY TEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    print("JewelryBox.AI CLI Test (type 'exit')")
    history = []
    while True:
        try:
            text = input("You: ").strip()
            if text.lower() in ("exit", "quit"): sys.exit(0)
            res = chain.invoke({"user_input": text, "history": history})
            reply = res.content.strip()
            print("JewelryBox.AI:", reply)
            memory.add_user_message(text)
            memory.add_ai_message(reply)
            history = memory.messages
        except KeyboardInterrupt:
            sys.exit(0)
        except Exception as e:
            print("Error:", e)
